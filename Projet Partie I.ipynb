{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1 Prétraitement/Baseline\n",
    "\n",
    "### Ouvrir les tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "1268\n",
      "1643\n",
      "494\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "import os\n",
    "\n",
    "def ouvrir(dossier):\n",
    "    données = []\n",
    "    g = os.walk(dossier)\n",
    "    for path,dir_list,file_list in g:  \n",
    "        for file_name in file_list:  \n",
    "            with open(os.path.join(path, file_name),'r',encoding=\"utf-8\") as f:\n",
    "                g1 = f.readline()\n",
    "                while g1!=\"\":\n",
    "                    données.append(g1.strip())\n",
    "                    g1 = f.readline()\n",
    "    return données\n",
    "\n",
    "mixed = ouvrir(\"mixed\")\n",
    "negatif = ouvrir(\"negative\")\n",
    "objectif = ouvrir(\"objective\")\n",
    "positif = ouvrir(\"positive\")\n",
    "\n",
    "print(len(mixed))\n",
    "print(len(negatif))\n",
    "print(len(objectif))\n",
    "print(len(positif))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "1268\n",
      "1643\n",
      "494\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokeniser(l):\n",
    "    données = []\n",
    "    for x in l:\n",
    "        données.append(word_tokenize(x,language=\"french\"))\n",
    "    return données\n",
    "\n",
    "mixed_tokeniser = tokeniser(mixed)\n",
    "negatif_tokeniser = tokeniser(negatif)\n",
    "objectif_tokeniser = tokeniser(objectif)\n",
    "positif_tokeniser = tokeniser(positif)\n",
    "\n",
    "    \n",
    "print(len(mixed_tokeniser))\n",
    "print(len(negatif_tokeniser))\n",
    "print(len(objectif_tokeniser))\n",
    "print(len(positif_tokeniser))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alors', 'au', 'aucuns', 'aussi', 'autre', 'avant', 'avec', 'avoir', 'bon', 'car']\n"
     ]
    }
   ],
   "source": [
    "stop_words_fr = []\n",
    "with open(\"../../../ZijianNLP/stop_words_fr.txt\",'r',encoding=\"utf-8\") as f:\n",
    "    g = f.readline()\n",
    "    while g!=\"\":\n",
    "        stop_words_fr.append(g.strip())\n",
    "        g = f.readline()\n",
    "        \n",
    "print(stop_words_fr[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2 Cross Validation sur TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorisation(l,stop_words_list):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(encoding = \"utf-8\",strip_accents='unicode',lowercase = True,stop_words=stop_words_list)\n",
    "    return vectorizer.fit_transform(l).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Préparer les données pour la vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que reste-t-il des anaphores de `` Moi , président de la république '' ... ? http : //fb.me/7But7qGNb\n"
     ]
    }
   ],
   "source": [
    "negatif = [\" \".join(x) for x in negatif_tokeniser]\n",
    "positif = [\" \".join(x) for x in positif_tokeniser]\n",
    "objectif = [\" \".join(x) for x in objectif_tokeniser]\n",
    "mixed = [\" \".join(x) for x in mixed_tokeniser]\n",
    "print(negatif[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorisation par CountVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(494, 2225)\n",
      "(1268, 5976)\n",
      "(1643, 7156)\n",
      "(3405, 11781)\n",
      "(3405, 11781)\n",
      "(3405,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(encoding = \"utf-8\",lowercase = True,stop_words=stop_words_fr)\n",
    "neg = vectorizer.fit_transform(negatif)\n",
    "pos = vectorizer.fit_transform(positif)\n",
    "obj = vectorizer.fit_transform(objectif)\n",
    "mix = vectorizer.fit_transform(mixed)\n",
    "neg_pos = vectorizer.fit_transform(negatif+positif)\n",
    "T = positif+negatif+objectif\n",
    "pos_neg_obj = vectorizer.fit_transform(T)\n",
    "\n",
    "import numpy as np\n",
    "positif_array = pos.toarray()\n",
    "negatif_array = neg.toarray()\n",
    "objectif_array = obj.toarray()\n",
    "commentaires = pos_neg_obj.toarray()\n",
    "print(positif_array.shape)\n",
    "print(negatif_array.shape)\n",
    "print(objectif_array.shape)\n",
    "print(commentaires.shape)\n",
    "\n",
    "X = commentaires\n",
    "print(X.shape)\n",
    "Y = [\"positif\" for x in range(pos.shape[0])]+[\"negatif\" for x in range(neg.shape[0])]+[\"objectif\" for x in range(obj.shape[0])]\n",
    "Y = np.asarray(Y)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des features après CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un extrait des features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quelques Features</th>\n",
       "      <th>Scores for the 200ist tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature 1</th>\n",
       "      <td>dorigine</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 2</th>\n",
       "      <td>dormir</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 3</th>\n",
       "      <td>doru</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 4</th>\n",
       "      <td>dorée</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 5</th>\n",
       "      <td>dose</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 6</th>\n",
       "      <td>dossier</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 7</th>\n",
       "      <td>douanier</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 8</th>\n",
       "      <td>double</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 9</th>\n",
       "      <td>doublé</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 10</th>\n",
       "      <td>doubs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 11</th>\n",
       "      <td>douce</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 12</th>\n",
       "      <td>douceur</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 13</th>\n",
       "      <td>doucher</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 14</th>\n",
       "      <td>douleur</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 15</th>\n",
       "      <td>douma</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 16</th>\n",
       "      <td>doute</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 17</th>\n",
       "      <td>doutent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 18</th>\n",
       "      <td>douter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 19</th>\n",
       "      <td>doux</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 20</th>\n",
       "      <td>doués</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Quelques Features  Scores for the 200ist tweet\n",
       "feature 1           dorigine                            0\n",
       "feature 2             dormir                            0\n",
       "feature 3               doru                            0\n",
       "feature 4              dorée                            0\n",
       "feature 5               dose                            0\n",
       "feature 6            dossier                            0\n",
       "feature 7           douanier                            0\n",
       "feature 8             double                            0\n",
       "feature 9             doublé                            0\n",
       "feature 10             doubs                            0\n",
       "feature 11             douce                            0\n",
       "feature 12           douceur                            1\n",
       "feature 13           doucher                            0\n",
       "feature 14           douleur                            0\n",
       "feature 15             douma                            0\n",
       "feature 16             doute                            0\n",
       "feature 17           doutent                            0\n",
       "feature 18            douter                            0\n",
       "feature 19              doux                            0\n",
       "feature 20             doués                            0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(\"Un extrait des features\")\n",
    "pd.DataFrame({\"Quelques Features\":vectorizer.get_feature_names()[4000:4020],\"Scores for the 200ist tweet\":X[200][4000:4020]},index = [\"feature {}\".format(i) for i in range(1,21)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour la Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation J48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3405, 11781)\n",
      "(3405,)\n",
      "Decision Tree :  0.6713615023474179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split as train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = commentaires\n",
    "print(X.shape)\n",
    "Y = [\"positif\" for x in range(pos.shape[0])]+[\"negatif\" for x in range(neg.shape[0])]+[\"objectif\" for x in range(obj.shape[0])]\n",
    "Y = np.asarray(Y)\n",
    "print(Y.shape)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state = 4)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,Y_train)\n",
    "\n",
    "clf_y_pred = clf.predict(X_test)\n",
    "treeScore = accuracy_score(Y_test,clf_y_pred)\n",
    "\n",
    "print(\"Decision Tree : \",treeScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70175439 0.71345029 0.66666667 0.73607038 0.66764706 0.64705882\n",
      " 0.67647059 0.70294118 0.72566372 0.7020649 ]\n",
      "0.6939787986889057\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf,X,Y,cv = 10, scoring = 'accuracy')\n",
    "print(scores)\n",
    "print(scores.mean())\n",
    "Decision_Tree_CountVector = scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualisation en PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data = tree.export_graphviz(clf,out_file=None,\n",
    "                                feature_names=vectorizer.get_feature_names(),\n",
    "                                class_names=['positif','negatif','objectif'],\n",
    "                                filled=True,rounded=True,\n",
    "                                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_pdf(\"Countvectorizer Arbre.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5481220657276995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train,Y_train)\n",
    "\n",
    "print(knn.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61143695 0.62316716 0.60557185 0.59558824 0.59351988]\n",
      "0.6058568141105946\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(knn,X,Y,cv = 5, scoring = 'accuracy')\n",
    "print(score)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_CountVector = score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation par Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes :  0.6848481069094847\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mnb = MultinomialNB(alpha=0.01).fit(X_train,Y_train)\n",
    "mnb_y_pred = mnb.predict(X_test)\n",
    "mnbScore = accuracy_score(Y_test,mnb_y_pred)\n",
    "score = cross_val_score(mnb,X,Y,cv = 10,scoring = \"accuracy\")\n",
    "\n",
    "NB_CountVector = score.mean()\n",
    "print(\"Naïve Bayes : \",NB_CountVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zijian/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6889671361502347\n",
      "Random Forest :  0.7165786163426002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier()\n",
    "RF = RF.fit(X_train,Y_train)\n",
    "\n",
    "RFScore = RF.score(X_test,Y_test)\n",
    "\n",
    "print(RFScore)\n",
    "score = cross_val_score(RF,X,Y,cv = 10,scoring = \"accuracy\")\n",
    "\n",
    "RF_CountVector = score.mean()\n",
    "print(\"Random Forest : \",RF_CountVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion avec la cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer : \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KNN</th>\n",
       "      <th>Naïve Bayes</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random_Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CountVectorizer</th>\n",
       "      <td>0.605857</td>\n",
       "      <td>0.684848</td>\n",
       "      <td>0.691948</td>\n",
       "      <td>0.716579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      KNN  Naïve Bayes  Decision Tree  Random_Forest\n",
       "CountVectorizer  0.605857     0.684848       0.691948       0.716579"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"CountVectorizer : \")\n",
    "pd.DataFrame({\"KNN\":KNN_CountVector,\"Naïve Bayes\":NB_CountVector,\"Decision Tree\":Decision_Tree_CountVector,\"Random_Forest\":RF_CountVector},index=[\"CountVectorizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 3. Cross Valid sur TEST \n",
    "\n",
    "-- Voir s'il existe du sur-apprentissage évident. 0%? 100%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ouvrir le test de tweet en json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"task1-testGold.csv.json\",'r') as load_f:\n",
    "     load_dict = json.load(load_f)\n",
    "\n",
    "objectif_test = []\n",
    "pos_test = []\n",
    "neg_test = []\n",
    "for x in load_dict[\"objective\"]:\n",
    "    objectif_test.append(x['content'])\n",
    "for x in load_dict[\"positive\"]:\n",
    "    pos_test.append(x['content'])\n",
    "for x in load_dict[\"negative\"]:\n",
    "    neg_test.append(x['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56321839 0.62790698 0.62790698 0.47058824 0.62352941 0.64705882\n",
      " 0.65882353 0.63529412 0.60714286 0.29761905]\n",
      "0.5759088366701932\n",
      "0.460093896713615\n",
      "[0.37931034 0.58139535 0.48837209 0.38823529 0.52941176 0.57647059\n",
      " 0.57647059 0.58823529 0.53571429 0.20238095]\n",
      "0.4845996554195054\n",
      "0.5774647887323944\n",
      "0.48593335489769274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zijian/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split as train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(encoding = \"utf-8\",lowercase = True,stop_words=stop_words_fr)\n",
    "X = vectorizer.fit_transform(pos_test+neg_test+objectif_test)\n",
    "Y = [\"positif\" for x in range(len(pos_test))]+[\"negatif\" for x in range(len(neg_test))]+[\"objectif\" for x in range(len(objectif_test))]\n",
    "X = X.toarray()\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state = 4)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,Y_train)\n",
    "\n",
    "clf_y_pred = clf.predict(X_test)\n",
    "treeScore = accuracy_score(Y_test,clf_y_pred)\n",
    "\n",
    "score = cross_val_score(clf,X,Y,cv = 10, scoring = 'accuracy')\n",
    "print(score)\n",
    "print(score.mean())\n",
    "CountVector_Test_DecisionTree = score.mean()\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,Y_train)\n",
    "print(knn.score(X_test,Y_test))\n",
    "\n",
    "score = cross_val_score(knn,X,Y,cv = 10, scoring = 'accuracy')\n",
    "print(score)\n",
    "print(score.mean())\n",
    "CountVector_Test_KNN = score.mean()\n",
    "\n",
    "mnb = MultinomialNB(alpha=0.01).fit(X_train,Y_train)\n",
    "mnb_y_pred = mnb.predict(X_test)\n",
    "mnbScore = accuracy_score(Y_test,mnb_y_pred)\n",
    "\n",
    "print(mnbScore)\n",
    "\n",
    "score = cross_val_score(mnb,X,Y,cv = 10,scoring = \"accuracy\")\n",
    "\n",
    "CountVector_Test_NB = score.mean()\n",
    "print(score.mean())\n",
    "\n",
    "RF = RandomForestClassifier()\n",
    "RF = RF.fit(X_train,Y_train)\n",
    "\n",
    "score = cross_val_score(RF,X,Y,cv = 10,scoring = \"accuracy\")\n",
    "\n",
    "RF_CountVectorize_test = score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pas de grande surprise pour les données de tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation pour les données de test pour éviter le sur-apprentissage: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KNN</th>\n",
       "      <th>Naïve_Bayes</th>\n",
       "      <th>J48</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CountVector</th>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.485933</td>\n",
       "      <td>0.575909</td>\n",
       "      <td>0.608009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                KNN  Naïve_Bayes       J48  Random Forest\n",
       "CountVector  0.4846     0.485933  0.575909       0.608009"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cross-Validation pour les données de test pour éviter le sur-apprentissage: \")\n",
    "pd.DataFrame({\"KNN\":[CountVector_Test_KNN],\"Naïve_Bayes\":[CountVector_Test_NB],\"J48\":[CountVector_Test_DecisionTree],\"Random Forest\":[RF_CountVectorize_test]},index = [\"CountVector\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 4. Vectorisation avec TFIDF et Implimentation\n",
    "\n",
    "Rather than just counting, we can use the TF-IDF score of a word to rank it's importance\n",
    "\n",
    "TFIDF score of a word, w, is\n",
    "\n",
    "tf(w) * idf(w)\n",
    "\n",
    "Where \n",
    "\n",
    "tf(w) = (Number of times the word appears in a document) / (Total number of words in the document)\n",
    "\n",
    "idf(w) = log(Number of documents / Number of documents that contain word w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utiliser Sklearn.features_extraction Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ouf je craignais être le plus vieux candidat à une élection sous la 5éme République ... mais @ alainjuppe remporte la palme ! Bravo ! # NB2017\n",
      "Que reste-t-il des anaphores de `` Moi , président de la république '' ... ? http : //fb.me/7But7qGNb\n",
      "# Russie - # Poutine : `` Le projet de grande # Eurasie est bien évidemment ouvert aux pays de l ’ # Europe '' http : //fb.me/62oFModNn\n"
     ]
    }
   ],
   "source": [
    "print(positif[0])\n",
    "print(negatif[0])\n",
    "print(objectif[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3405, 11781)\n",
      "(3405,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(encoding = \"utf-8\",lowercase = True,stop_words=stop_words_fr)\n",
    "\n",
    "X = tfidf_vect.fit_transform(positif+negatif+objectif)\n",
    "\n",
    "#X = tfidf_vect.transform(positif+negatif+objectif)\n",
    "X = X.toarray()\n",
    "print(X.shape)\n",
    "\n",
    "Y = [\"positif\" for x in range(len(positif))]+[\"négatif\" for x in range(len(negatif))]+[\"objectif\" for x in range(len(objectif))]\n",
    "Y = np.asarray(Y)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualiser quelques features après TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quelques Features</th>\n",
       "      <th>Scores for the 200ist tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature 1</th>\n",
       "      <td>dorigine</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 2</th>\n",
       "      <td>dormir</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 3</th>\n",
       "      <td>doru</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 4</th>\n",
       "      <td>dorée</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 5</th>\n",
       "      <td>dose</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 6</th>\n",
       "      <td>dossier</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 7</th>\n",
       "      <td>douanier</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 8</th>\n",
       "      <td>double</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 9</th>\n",
       "      <td>doublé</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 10</th>\n",
       "      <td>doubs</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 11</th>\n",
       "      <td>douce</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 12</th>\n",
       "      <td>douceur</td>\n",
       "      <td>0.291795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 13</th>\n",
       "      <td>doucher</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 14</th>\n",
       "      <td>douleur</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 15</th>\n",
       "      <td>douma</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 16</th>\n",
       "      <td>doute</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 17</th>\n",
       "      <td>doutent</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 18</th>\n",
       "      <td>douter</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 19</th>\n",
       "      <td>doux</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature 20</th>\n",
       "      <td>doués</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Quelques Features  Scores for the 200ist tweet\n",
       "feature 1           dorigine                     0.000000\n",
       "feature 2             dormir                     0.000000\n",
       "feature 3               doru                     0.000000\n",
       "feature 4              dorée                     0.000000\n",
       "feature 5               dose                     0.000000\n",
       "feature 6            dossier                     0.000000\n",
       "feature 7           douanier                     0.000000\n",
       "feature 8             double                     0.000000\n",
       "feature 9             doublé                     0.000000\n",
       "feature 10             doubs                     0.000000\n",
       "feature 11             douce                     0.000000\n",
       "feature 12           douceur                     0.291795\n",
       "feature 13           doucher                     0.000000\n",
       "feature 14           douleur                     0.000000\n",
       "feature 15             douma                     0.000000\n",
       "feature 16             doute                     0.000000\n",
       "feature 17           doutent                     0.000000\n",
       "feature 18            douter                     0.000000\n",
       "feature 19              doux                     0.000000\n",
       "feature 20             doués                     0.000000"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"Quelques Features\":vectorizer.get_feature_names()[4000:4020],\"Scores for the 200ist tweet\":X[200][4000:4020]},index = [\"feature {}\".format(i) for i in range(1,21)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraîner par des modèles\n",
    "\n",
    "#####  Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IFIDF Decision Tree :  0.6772300469483568\n",
      "[0.71052632 0.72222222 0.65789474 0.73020528 0.65       0.64705882\n",
      " 0.67352941 0.72058824 0.72271386 0.69616519]\n",
      "0.693090408008161\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state = 4)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,Y_train)\n",
    "\n",
    "clf_y_pred = clf.predict(X_test)\n",
    "treeScore = accuracy_score(Y_test,clf_y_pred)\n",
    "\n",
    "print(\"IFIDF Decision Tree : \",treeScore)\n",
    "\n",
    "# Cross-Validation\n",
    "score = cross_val_score(clf,X,Y,cv = 10, scoring = 'accuracy')\n",
    "print(score)\n",
    "print(score.mean())\n",
    "TFIDF_DecisionTree = score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation du résultat de DécisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data = tree.export_graphviz(clf,out_file=None,\n",
    "                                feature_names=tfidf_vect.get_feature_names(),\n",
    "                                class_names=['positif','negatif','objectif'],\n",
    "                                filled=True,rounded=True,\n",
    "                                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_pdf(\"TFIDF DecisionTree Arbre.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6678403755868545\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train,Y_train)\n",
    "\n",
    "print(knn.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70760234 0.65497076 0.67251462 0.69501466 0.65294118 0.69411765\n",
      " 0.66176471 0.7        0.7079646  0.68141593]\n",
      "0.6828306442440061\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(knn,X,Y,cv = 10, scoring = 'accuracy')\n",
    "print(score)\n",
    "print(score.mean())\n",
    "TFIDF_KNN = score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653755868544601\n",
      "Naïve Bayes :  0.6860374748725467\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB(alpha=0.01).fit(X_train,Y_train)\n",
    "mnb_y_pred = mnb.predict(X_test)\n",
    "mnbScore = accuracy_score(Y_test,mnb_y_pred)\n",
    "\n",
    "print(mnbScore)\n",
    "\n",
    "score = cross_val_score(mnb,X,Y,cv = 10,scoring = \"accuracy\")\n",
    "\n",
    "TFIDF_NB = score.mean()\n",
    "print(\"Naïve Bayes : \",TFIDF_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zijian/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.710093896713615\n",
      "Random Forest :  0.7200951154220886\n"
     ]
    }
   ],
   "source": [
    "RF = RandomForestClassifier()\n",
    "RF = RF.fit(X_train,Y_train)\n",
    "\n",
    "RFScore = RF.score(X_test,Y_test)\n",
    "\n",
    "print(RFScore)\n",
    "score = cross_val_score(RF,X,Y,cv = 10,scoring = \"accuracy\")\n",
    "\n",
    "RF_TFIDF = score.mean()\n",
    "print(\"Random Forest : \",RF_TFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Résultats Obtenus Avec la cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation pour l'apprentissage: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KNN</th>\n",
       "      <th>Naïve Bayes</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CountVectorizer</th>\n",
       "      <td>0.605857</td>\n",
       "      <td>0.684848</td>\n",
       "      <td>0.691948</td>\n",
       "      <td>0.716579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFIDF</th>\n",
       "      <td>0.682831</td>\n",
       "      <td>0.686037</td>\n",
       "      <td>0.693090</td>\n",
       "      <td>0.720095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      KNN  Naïve Bayes  Decision Tree  Random Forest\n",
       "CountVectorizer  0.605857     0.684848       0.691948       0.716579\n",
       "TFIDF            0.682831     0.686037       0.693090       0.720095"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cross-Validation pour l'apprentissage: \")\n",
    "pd.DataFrame({\"KNN\":[KNN_CountVector,TFIDF_KNN],\"Naïve Bayes\":[NB_CountVector,TFIDF_NB],\"J48\":[Decision_Tree_CountVector,TFIDF_DecisionTree],\"Random Forest\":[RF_CountVector,RF_TFIDF]},index=[\"CountVectorizer\",\"TFIDF\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donc, nous utilisons Decision Tree et Random Forest pour un test avec TFIDF qui se montre un peu meilleur que les autres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### le score final pour le CountVectoriser, TFIDF\n",
    "\n",
    "### Nous choisissons J48 et Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494   123\n",
      "1268   318\n",
      "1643   411\n",
      "4257\n",
      "4257\n",
      "(3405, 13638) (3405,)\n",
      "(852, 13638) (852,)\n",
      "0.6549295774647887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zijian/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3405, 13638) (3405,)\n",
      "(852, 13638) (852,)\n",
      "0.6866197183098591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zijian/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print(len(positif),\" \",len(pos_test))\n",
    "print(len(negatif),\" \",len(neg_test))\n",
    "print(len(objectif),\" \",len(objectif_test))\n",
    "\n",
    "Total = positif+negatif+objectif+pos_test+neg_test+objectif_test\n",
    "Target = [\"positif\" for x in range(len(positif))]+[\"negatif\" for x in range(len(negatif))]+[\"objectif\" for x in range(len(objectif))]+[\"positif\" for x in range(len(pos_test))]+[\"negatif\" for x in range(len(neg_test))]+[\"objectif\" for x in range(len(objectif_test))]\n",
    "print(len(Total))\n",
    "print(len(Target))\n",
    "\n",
    "vectorizer = CountVectorizer(encoding = \"utf-8\",lowercase = True,stop_words=stop_words_fr)\n",
    "\n",
    "X = vectorizer.fit_transform(Total)\n",
    "X_train = X[:(len(positif)+len(negatif)+len(objectif))]\n",
    "X_test = X[(len(positif)+len(negatif)+len(objectif)):]\n",
    "\n",
    "Y_train = Target[:3405]\n",
    "Y_test = Target[3405:]\n",
    "\n",
    "Y_train = np.asarray(Y_train)\n",
    "Y_test = np.asarray(Y_test)\n",
    "\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n",
    "# J48\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,Y_train)\n",
    "\n",
    "clf_y_pred = clf.predict(X_test)\n",
    "treeScore = accuracy_score(Y_test,clf_y_pred)\n",
    "\n",
    "Count_vect_J48 = treeScore\n",
    "\n",
    "print(Count_vect_J48)\n",
    "\n",
    "RF = RandomForestClassifier()\n",
    "RF = RF.fit(X_train,Y_train)\n",
    "\n",
    "Count_vect_RF = RF.score(X_test,Y_test)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(encoding = \"utf-8\",lowercase = True,stop_words=stop_words_fr)\n",
    "\n",
    "X = tfidf_vect.fit_transform(Total)\n",
    "X_train = X[:(len(positif)+len(negatif)+len(objectif))]\n",
    "X_test = X[(len(positif)+len(negatif)+len(objectif)):]\n",
    "\n",
    "Y_train = Target[:3405]\n",
    "Y_test = Target[3405:]\n",
    "\n",
    "Y_train = np.asarray(Y_train)\n",
    "Y_test = np.asarray(Y_test)\n",
    "\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,Y_train)\n",
    "\n",
    "clf_y_pred = clf.predict(X_test)\n",
    "treeScore = accuracy_score(Y_test,clf_y_pred)\n",
    "\n",
    "TFIDF_J48= treeScore\n",
    "\n",
    "print(TFIDF_J48)\n",
    "\n",
    "RF = RandomForestClassifier()\n",
    "RF = RF.fit(X_train,Y_train)\n",
    "\n",
    "TFIDF_RF = RF.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le résultat global pour CountVect et TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>J48</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CountVectore</th>\n",
       "      <td>0.65493</td>\n",
       "      <td>0.686620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFIDF</th>\n",
       "      <td>0.70892</td>\n",
       "      <td>0.705399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  J48  Random Forest\n",
       "CountVectore  0.65493       0.686620\n",
       "TFIDF         0.70892       0.705399"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"J48\":[Count_vect_J48,Count_vect_RF],\"Random Forest\":[TFIDF_J48,TFIDF_RF]},index=[\"CountVectore\",\"TFIDF\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 5. Okapi BM 25 \n",
    "\n",
    "Dans le deuxième jupyter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
